# -*- coding: utf-8 -*-
"""Llama3_finetuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gU5WVT0BP0pdX4Qso1mq06L2Obqwrtzf
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     # Do this only in Colab notebooks! Otherwise use pip install unsloth
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

#load model
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # New Mistral v3 2x faster!
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3 15 trillion tokens model 2x faster!
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3 2x faster!
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemma 2.2x faster!
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit", # "unsloth/llama-3-8b-bnb-4bit loading
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
)

model = FastLanguageModel.get_peft_model(
    model,
    r = 128,
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",
                      "embed_tokens", "lm_head"],
    lora_alpha = 32,

    lora_dropout = 0,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3407,

    use_rslora = True,
    loftq_config = None,
)

from datasets import load_dataset

# Load your custom dataset
eye_qa_plus = load_dataset("QIAIUNCC/EYE-QA-PLUS", split="train[:2000]")  # Adjust the split as needed

# Split into train and test sets (80-20 split)
eye_qa_plus = eye_qa_plus.train_test_split(test_size=0.2)

text = eye_qa_plus['train'][0]
text

from datasets import load_dataset
from unsloth import UnslothTrainer, UnslothTrainingArguments, FastLanguageModel
from transformers import DataCollatorForLanguageModeling

# Load your custom dataset
eye_qa_plus = load_dataset("QIAIUNCC/EYE-QA-PLUS", split="train[:2000]")

# Split into train and test sets (80-20 split)
eye_qa_plus = eye_qa_plus.train_test_split(test_size=0.2)

# Preprocess
def preprocess_function(examples):
    inputs = examples["input"]  # Assuming this contains the context or instruction
    outputs = examples["output"]  # Assuming this contains the answer or output text
    instructions = examples["instruction"]

    # Format the input in a suitable way for your model
    input_text = [f"Instruction: {instruction} {input}" for instruction, input in zip(instructions, inputs)]
    target_text = outputs

    return {"input_text": input_text, "target_text": target_text}

# Apply preprocessing to the dataset
eye_qa_plus_processed = eye_qa_plus.map(preprocess_function, batched=True, num_proc=8, remove_columns=["input", "output", "instruction", "source"])

# Formatting with stop token
def formatting_func(example):
    # Return the formatted string as a dictionary
    return {"input_text": f"{example['input_text']} Answer: {example['target_text']}<|eot|>"}

# Apply formatting function
eye_qa_plus_processed = eye_qa_plus_processed.map(formatting_func, batched=False)

eye_qa_plus_processed["train"][0]
formatting_func(eye_qa_plus_processed["train"][1])

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported
# Data collator
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #not using mask language

# Trainer
trainer = UnslothTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=eye_qa_plus_processed["train"],
    dataset_text_field="input_text",
    formatting_func=formatting_func,
    max_seq_length=512,
    data_collator=data_collator,
    args=UnslothTrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=8,
        warmup_ratio=0.1,
        num_train_epochs=3,
        learning_rate=5e-5,
        embedding_learning_rate=5e-6,
        fp16 = not is_bfloat16_supported(), # Use 16-bit floating point if bfloat16 isn't supported.
        bf16 = is_bfloat16_supported(),     #use bf16 if hardware support
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.00,
        lr_scheduler_type="cosine", #Cosine scheduler to adjust the learning rate over time.
        seed=3407,
        output_dir="outputs",
        report_to="none",
    ),
)

# Train
trainer.train()

#saved the finetuned model
model.save_pretrained("outputs")
tokenizer.save_pretrained("outputs")

!zip -r opticlarity_finetuned_model.zip outputs/

from google.colab import files
files.download("opticlarity_finetuned_model.zip")